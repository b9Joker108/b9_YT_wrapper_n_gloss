# Personal and programmable YouTube algorithm, dashboard with knowledge graph visualisation and tooling #

I have had it with the unintelligent, unprogrammable, timewasting blackbox that is the YouTube recommendation algorithm. It does not furnish my requirements. This project is to create a personal workaround with some bells-and-whistles. This personal hack project is a fork of the formative work in Python done by Chris Lovejoy, refer [post on Chris' weblog](https://chrislovejoy.me/youtube-algorithm) and [Chris's repository](https://github.com/chris-lovejoy/YouTube-video-finder) of same and refer [recent work by Gauri Joshi, *et al.*](https://github.com/rosadiaznewyork/video-finder-algorithm) and her [YouTube overview](https://youtu.be/r5Y1I1Zol2k?si=ScYVKV89Q950sAmo) of same. Joshi is an Associate Professor in the Electrical and Computer Engineering (ECE) Department of Carnegie Mellon University (CMU), Pittsburgh, Pennsylvania, U.S.A. and has published a recent monograph in 2023 with Springer, no less: 'Optimization Algorithms for Distributed Machine Learning' and a number of peer-reviewed article collaborations. So, Prof. Joshi's code and state-of-the-art working knowledge on programmable algorithms clearly holds weight and her repository demonstrates a structural finesse at first glance. I might be a bit of a fanboy of Prof. Joshi in truth. 

My project goals are different to Chris' and Gauri's as may be garnered from my [project_weblog](/project_weblog/project_weblog.md). I actively searched for and discovered Chris' project and subsequently came upon Gauri's through fortuitous happenstance by personal feed of YouTube's recommendation algorithm, which is the very stuff of irony. Given the explosion of GenAI generated or augmented content on YouTube, the lion's share of which I consider signal noise, I need a robust way to actively filter out this and other guff and excavate and foreground what I value. This project, too, preempts the soon-to-arise multimodal GenAI audiovisual generation tools that will generate personally curated content, according to personal interest and requirements, in realtime and on-the-fly, as it will provide a dashboard to curate this future potentiality of generated dynamic, responsive, on-demand, audiovisual content (refer [Preparing for Possible Futures](/project_weblog/preparing_for_possible_futures.md)). I favour Python, JavaScript and shell programming on an Android tablet interface, integrated into a Debian GNU/Linux home server, foregrounding Termux and/or a Debian GNU/Linux userlands and environments and these will delimit and delineate the project. I depend on Vibe Coding and GenAI coding agents to assist with my coding endeavours as my coding kungfu is rather rudimentary. As a writer, researcher and translator, I use a specification to drive my endeavours, as standard, but it is often *ad hoc* and not formalised as I generally work on solo projects. So, I perceive the real value of frameworks such as [GitHub's Spec-kit](https://github.com/github/spec-kit), a toolkit which helps to structure and formalise a working software development project specification that is human- and machine-readable that constitutes an unambiguous, dynamic evergreen or living document and itemised source-of-truth to drive the productivity of human, GenAI vibe and GenAI agentic coding teams and systems with onboarding, ongoing undertakings and operational directives and the coding of greenfield and mature coding projects with the precision of emergent processes, techniques and refinements of Human/GenAI Spec-Driven Development and collaboration. On the face of it, GitHub's Spec-kit inaugurates a new Python project with a Python UV integration, but does not appear to yet have the propensity to retrofit established projects and may not in future, as retrofitting may compromise the driving precision of the `spec-kit` framework established a inauguration of a greenfield human and GenAI agentic software develooment and coding projects. The `spec-kit` is fundamentally add-water-and-stir in ethos to enforce project ontology and specification standardisation to delimit congruence from ambiguity. Retrofitting is antithetical to this ethos and its theoretical foundation and design principles. From what I can garner, `spec-kit` is a CLI application and specification generation, maintenance and updating framework that integrates with the `uv` Python project's subproject `uva` and `postgres`, which I favour and `redis`, to which I am not privy and continually redefines the evergreen specifications according to changing requirements and developments. As the evergreen specifications in Markdown are within an environment managed by `git`, all states of the specifications are timestamped and controlled according to project governance, hence, are discoverable and auditable for probity, quality-control and future-proofing. The `*.md` specs are human readable and through process-augmentation are made dynamically ML readable. So, developing via the computational terminal and CLI becomes streamlined as all humans and GenAI prompting and GenAI agentic processes and systems access the same specifications as the source-of-truth driving development. I understand now that `redis` is like a resource- and cost-saving working short-term memory for the long-term memory nested as vectors in a `postgres` DB whose source-of-truth are the formalised and evergreen and living Markdown specifications for the software/coding project in question housed in a given code repository.


